{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNGhro/Su9KPVSFzOmxgut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShantanuKadam3115/MachineLearningBasics/blob/ML_implementations/3layerDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljAzfiiB9ZB8",
        "outputId": "3143a62c-6f62-45dc-cd01-51bd171d7380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Setup ---\n",
            "Input x:\n",
            "[[ 1 -1]]\n",
            "Shape of x: (1, 2) (1 row, 2 columns)\n",
            "Target y: [2]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np  # Import the math library\n",
        "\n",
        "# 1. Define the input x\n",
        "# numpy.array creates a matrix/vector\n",
        "x = np.array([[1, -1]])\n",
        "\n",
        "# 2. Define the target y (Class 2)\n",
        "# We put it in a list [] because usually we have a batch of labels\n",
        "y = np.array([2])\n",
        "\n",
        "print(\"--- Data Setup ---\")\n",
        "print(f\"Input x:\\n{x}\")\n",
        "print(f\"Shape of x: {x.shape} (1 row, 2 columns)\")\n",
        "print(f\"Target y: {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TalkativeThreeLayerNet:\n",
        "    # __init__ is the \"Constructor\". It runs once when you create the object.\n",
        "    def __init__(self, input_size, h1, h2, output_size):\n",
        "        self.params = {} # A dictionary to hold our weights\n",
        "\n",
        "        # Initialize weights with the exact numbers from our paper exercise\n",
        "        # We use .astype(float) to make sure Python treats them as decimals, not integers\n",
        "        self.params['W1'] = np.array([[1, 1], [1, -1], [-1, 0]]).astype(float)\n",
        "        self.params['b1'] = np.zeros(3) # Zeros vector of size 3\n",
        "\n",
        "        self.params['W2'] = np.array([[1, 0, 0], [0, 0.5, 0], [1, 1, 1]]).astype(float)\n",
        "        self.params['b2'] = np.zeros(3)\n",
        "\n",
        "        self.params['W3'] = np.array([[0, 0, 0], [0, 1, 0], [0, -1, 1], [0, 0, -1]]).astype(float)\n",
        "        self.params['b3'] = np.zeros(4)\n",
        "\n",
        "    def forward_pass(self, X, verbose=False):\n",
        "        # Unpack weights from the dictionary for easier typing\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        W3, b3 = self.params['W3'], self.params['b3']\n",
        "\n",
        "        # --- LAYER 1 ---\n",
        "        # .dot() is Matrix Multiplication\n",
        "        # .T means Transpose (flip rows/cols)\n",
        "        z1 = X.dot(W1.T) + b1\n",
        "        h1 = np.maximum(0, z1) # ReLU function\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[Layer 1] Input shape: {X.shape}, W1 shape: {W1.shape}\")\n",
        "            print(f\"z1 (Pre-activation):\\n{z1}\")\n",
        "            print(f\"h1 (Post-ReLU):\\n{h1}\")\n",
        "\n",
        "        # --- LAYER 2 ---\n",
        "        z2 = h1.dot(W2.T) + b2\n",
        "        h2 = np.maximum(0, z2)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[Layer 2] W2 shape: {W2.shape}\")\n",
        "            print(f\"z2 (Pre-activation):\\n{z2}\")\n",
        "            print(f\"h2 (Post-ReLU):\\n{h2}\")\n",
        "\n",
        "        # --- LAYER 3 ---\n",
        "        z3 = h2.dot(W3.T) + b3\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[Layer 3] W3 shape: {W3.shape}\")\n",
        "            print(f\"z3 (Final Scores):\\n{z3}\")\n",
        "\n",
        "        # Store values in 'cache' because we need them for Backprop later\n",
        "        cache = (z1, h1, z2, h2, z3)\n",
        "        return z3, cache\n",
        "\n",
        "    def backward_pass(self, X, y, cache, verbose=False):\n",
        "        # Unpack our saved values\n",
        "        z1, h1, z2, h2, z3 = cache\n",
        "\n",
        "        # Unpack weights again\n",
        "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
        "        N = X.shape[0] # Number of images (1 in our case)\n",
        "\n",
        "        # 1. Softmax + Loss Calculation\n",
        "        # shift values to avoid explosion (math trick)\n",
        "        shifted = z3 - np.max(z3, axis=1, keepdims=True)\n",
        "        exp_scores = np.exp(shifted)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # Calculate Loss\n",
        "        correct_logprobs = -np.log(probs[range(N), y])\n",
        "        loss = np.sum(correct_logprobs) / N\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n--- Backward Pass Starts ---\")\n",
        "            print(f\"Probabilities: {probs}\")\n",
        "            print(f\"Loss: {loss:.4f}\")\n",
        "\n",
        "        # 2. Gradient Calculation (The Blame Game)\n",
        "        grads = {}\n",
        "\n",
        "        # -- Gradient at Output --\n",
        "        dscores = probs\n",
        "        dscores[range(N), y] -= 1 # Subtract 1 from correct class\n",
        "        dscores /= N\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"dL/dz3 (dscores):\\n{dscores}\")\n",
        "\n",
        "        # -- Backprop Layer 3 --\n",
        "        grads['W3'] = dscores.T.dot(h2)\n",
        "        dh2 = dscores.dot(W3) # Gradient flowing to hidden layer 2\n",
        "\n",
        "        # -- Backprop ReLU 2 --\n",
        "        # (z2 > 0) creates a mask of True/False.\n",
        "        # Multiplication acts as the \"Gate\"\n",
        "        dz2 = dh2 * (z2 > 0)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\ndL/dh2 (Gradient at Hidden 2):\\n{dh2}\")\n",
        "            print(f\"dL/dz2 (After ReLU Gate):\\n{dz2}\")\n",
        "\n",
        "        # -- Backprop Layer 2 --\n",
        "        grads['W2'] = dz2.T.dot(h1)\n",
        "        dh1 = dz2.dot(W2)\n",
        "\n",
        "        # -- Backprop ReLU 1 --\n",
        "        dz1 = dh1 * (z1 > 0)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\ndL/dh1 (Gradient at Hidden 1):\\n{dh1}\")\n",
        "            print(f\"dL/dz1 (After ReLU Gate):\\n{dz1}\")\n",
        "\n",
        "        # -- Backprop Layer 1 --\n",
        "        grads['W1'] = dz1.T.dot(X)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nFinal Result: dL/dW1:\\n{grads['W1']}\")\n",
        "\n",
        "        return loss, grads"
      ],
      "metadata": {
        "id": "SwHd-cbDyJ13"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the network\n",
        "net = TalkativeThreeLayerNet(input_size=2, h1=3, h2=3, output_size=4)\n",
        "\n",
        "# 2. Run Forward Pass\n",
        "# We capture the scores and the 'cache' (memory of intermediate steps)\n",
        "print(\">>> RUNNING FORWARD PASS >>>\")\n",
        "scores, cache = net.forward_pass(x, verbose=True)\n",
        "\n",
        "# 3. Run Backward Pass\n",
        "print(\"\\n>>> RUNNING BACKWARD PASS >>>\")\n",
        "loss, grads = net.backward_pass(x, y, cache, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUztajrb0ZcO",
        "outputId": "21ff422d-571b-413a-c2ff-b8744d54305c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> RUNNING FORWARD PASS >>>\n",
            "\n",
            "[Layer 1] Input shape: (1, 2), W1 shape: (3, 2)\n",
            "z1 (Pre-activation):\n",
            "[[ 0.  2. -1.]]\n",
            "h1 (Post-ReLU):\n",
            "[[0. 2. 0.]]\n",
            "\n",
            "[Layer 2] W2 shape: (3, 3)\n",
            "z2 (Pre-activation):\n",
            "[[0. 1. 2.]]\n",
            "h2 (Post-ReLU):\n",
            "[[0. 1. 2.]]\n",
            "\n",
            "[Layer 3] W3 shape: (4, 3)\n",
            "z3 (Final Scores):\n",
            "[[ 0.  1.  1. -2.]]\n",
            "\n",
            ">>> RUNNING BACKWARD PASS >>>\n",
            "\n",
            "--- Backward Pass Starts ---\n",
            "Probabilities: [[0.15216302 0.41362198 0.41362198 0.02059303]]\n",
            "Loss: 0.8828\n",
            "dL/dz3 (dscores):\n",
            "[[ 0.15216302  0.41362198 -0.58637802  0.02059303]]\n",
            "\n",
            "dL/dh2 (Gradient at Hidden 2):\n",
            "[[ 0.          1.         -0.60697105]]\n",
            "dL/dz2 (After ReLU Gate):\n",
            "[[ 0.          1.         -0.60697105]]\n",
            "\n",
            "dL/dh1 (Gradient at Hidden 1):\n",
            "[[-0.60697105 -0.10697105 -0.60697105]]\n",
            "dL/dz1 (After ReLU Gate):\n",
            "[[-0.         -0.10697105 -0.        ]]\n",
            "\n",
            "Final Result: dL/dW1:\n",
            "[[ 0.          0.        ]\n",
            " [-0.10697105  0.10697105]\n",
            " [ 0.          0.        ]]\n"
          ]
        }
      ]
    }
  ]
}